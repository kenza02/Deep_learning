{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW4ZxVd1n7Kd"
      },
      "source": [
        "# Atelier 2 : Vectorisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT1wV-Don7Kf"
      },
      "source": [
        "## 1.\tObjectif\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P20HNzoCn7Kf"
      },
      "source": [
        "L‚Äôobjectif de cet atelier est de decouvrire les techniques courantes de vectorisation de documents text ansi que les differents algorithmes permettant de caculer la similarit√© ou la distance entre les document texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aCCUzLyn7Kg"
      },
      "source": [
        "## 2. M√©thodes Basiques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS-xpif2n7Kg"
      },
      "source": [
        "Afin de simplifier l‚Äôanalyse des donn√©es texte, il est recommand√© d‚Äôutiliser des repr√©sentations plus consistantes qu‚Äôune simple segmentation. Il faut bien √©videment r√©aliser des pr√©traitements telles que la racinisation, la lemmatisation, supprimer les redondances, supprimer les mots qui repr√©sentent le m√™me sens. Mais c‚Äôest encore insuffisant pour obtenir un mod√®le repr√©sentatif qui refl√®te l‚Äôimportance et le sens exacte de chaque mot dans une expression ou dans un document texte.\n",
        "Afin de repondre √† ce besoin, plusieurs repr√©sentations vectorielles des termes contenus dans un texte sont possibles : one-hot-vector, Bag-of-words, TF-IDF, SVD, Word2vec... Nous utilisant scikitlearn pour r√©aliser ces diff√©rentes repr√©sentations vectorielles. √áa n‚Äôemp√™che pas que ces repr√©sentations vectorielles peuvent √™tre obtenues en faisant du codage from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ITkj6bbn7Kh"
      },
      "source": [
        "## 2.1. One-hot-vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHi6XziRn7Kh"
      },
      "source": [
        "Le mod√®le commence par la cr√©ation d‚Äôun vocabulaire √† partir du corpus form√© par tous les documents ou les expressions texte et determine par la suite pour chaque document/expression la pr√©sence de chaque terme du vocabulaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFpfILnQn7Kh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "freq   = CountVectorizer()\n",
        "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
        "corpus=[ sent.lower() for sent in corpus]\n",
        "corpus = freq.fit_transform(corpus)\n",
        "\n",
        "onehot = Binarizer()\n",
        "corpus = onehot.fit_transform(corpus.toarray())\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUpeuGeSn7Ki"
      },
      "source": [
        "## 2.2.\tBeg-of-words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGz_XGYvn7Kj"
      },
      "source": [
        "Pour la repr√©sentation vectorielle Bag-of-Words, le mod√®le commence par la cr√©ation d‚Äôun vocabulaire √† partir du corpus form√© par tous les documents ou les expressions texte et calcul par la suite pour chaque document/expression le nombre d‚Äôoccurrences de chaque terme du vocabulaire.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNIDwQCxn7Kj",
        "outputId": "60b44742-e88b-4bf7-c04e-50b74301f764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 1 0 1 0 1 1 0 1]\n",
            " [1 0 0 0 1 0 1 1 0]\n",
            " [0 1 1 1 0 0 1 0 1]]\n",
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = ['This is the first document.','This is the second  document.','And the third one.','Is this the first document?']\n",
        "corpus=[ sent.lower() for sent in corpus]\n",
        "X = vectorizer.fit_transform(corpus) #sparsy format\n",
        "print(X.toarray()) # explicit matrix format\n",
        "print(vectorizer.get_feature_names() ) #vocabulary as list of string\n",
        "print(vectorizer.vocabulary_.get('document')) #get column index of a specific term in the vocabulary\n",
        "vectorizer.transform(['Something completely new.']).toarray()#apply the model to a new document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0zkMHOin7Kj"
      },
      "source": [
        "## 2.3.\tTF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCqlEza-n7Kj"
      },
      "source": [
        "La repr√©sentation vectorielle Bag-of-Words consid√®re la fr√©quence d‚Äôapparition des termes du vocabulaire dans chaque document du corpus s√©par√©ment des autres. Cette repr√©sentation n√©glige l‚Äôimportance du terme par rapport au corpus tout en entier.  TF-IDF (term-frequency times inverse document-frequency) est un autre mod√®le de repr√©sentation vectorielle des occurrences des termes d‚Äôun document en consid√©rant √©galement leurs occurrences dans tout le corpus. Cette approche va permettre de diminuer l‚Äôimportance des termes les plus fr√©quents dans des documents texte tels que les stop words.\n",
        "\n",
        "tf-idf(t,d)=tf(t,d)√óidf(t).\n",
        "\n",
        "idf(t)=log[(1+n)/(1+df(t))]+1\n",
        "n: la taille du corpus.\n",
        "df(t) :  le nombre de documents qui comportent le terme t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXn0f4ACn7Kk",
        "outputId": "8019d6dd-8cc1-412a-e3fc-e45afa4935db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
            "  0.35872874 0.         0.43877674]\n",
            " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
            "  0.22262429 0.         0.27230147]\n",
            " [0.55280532 0.         0.         0.         0.55280532 0.\n",
            "  0.28847675 0.55280532 0.        ]\n",
            " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
            "  0.35872874 0.         0.43877674]]\n",
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = ['This is the first document.','this is the second second Document.','And the third one.','Is this the first document?']\n",
        "vectorizer = TfidfVectorizer()\n",
        "X=vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        "print(vectorizer.get_feature_names() )\n",
        "\n",
        "#NB: le vecteur tf-idf obtenu sera normalis√© pour obtenir des valeurs entre 0 et 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_htBOtQn7Kk"
      },
      "source": [
        "## 2.4. Exercice 1: Detection du plagiarisme Approche syntaxique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBr2xsRZn7Kk"
      },
      "source": [
        "On va reprendre l'exercie de detection du plagiarisme en se bsant sur une representation vectorielle des document du corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaSxi4MQn7Kk"
      },
      "source": [
        "Pour la similarit√© syntaxique entre des vecteurs, plusieurs distances sont possibles √† savoir : distance, euclidienne, Cosine Jaccard, Levenshtein, Hamming‚Ä¶\n",
        "\n",
        "l'exemple ci-dessous permet de calculer la similarit√© syntaxique entre les documents en se basant sur une representation vectorielle en TFIDF avec la distance euclidienne et la distance cosine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ghsOP4mn7Kk",
        "outputId": "90940c60-4a34-4a2a-b28c-b32bbc462761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         1.05990529 1.33904078 0.        ]]\n",
            "[[0.         0.56169962 0.8965151  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances,euclidean_distances\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "tfidf_matrix.shape\n",
        "\n",
        "#compute similarity for first sentence with rest of the sentences\n",
        "print(euclidean_distances(tfidf_matrix[0:1],tfidf_matrix))\n",
        "print(cosine_distances(tfidf_matrix[0:1],tfidf_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmEHJhf2n7Kl"
      },
      "source": [
        "* Realiser la m√™me chose en se basant sur les representations OHV et BOW?\n",
        "* Comparer les performances des trois methodes en terme de temps d'execution et precision?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F02K82EFn7Km"
      },
      "source": [
        "# 3.\tMethodes Avanc√©es\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wopGemCzn7Kp"
      },
      "source": [
        "* Installer Gensim: pip install --upgrade gensim\n",
        "* Recuperer le dataset du plagiarisme?\n",
        "* R√©aliser les diff√©rentes t√¢ches de pr√©traitement?\n",
        "* Generer le vocabulaire sous forme d'une liste de termes distincts est tri√©s dans l'ordre lexicograpique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp7kUitTn7Kp"
      },
      "source": [
        "## 3.2.\tApproche √† base des Cooccurrences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJTgD2S8n7Kp"
      },
      "source": [
        "* Generer le vocabulaire sous forme d'une liste de termes distincts est tri√©s dans l'ordre lexicograpique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9pGmPqgn7Kp",
        "outputId": "5217cd4e-3b5a-4269-a904-2efa5a50fe9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['and', 'document', 'first', 'is', 'one', 'second', 'third', 'this']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "lemmmatizer=WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
        "corpus_lemetized=[]\n",
        "\n",
        "for doc in corpus:\n",
        "    words = word_tokenize(doc)\n",
        "    words = [lemmmatizer.lemmatize(word.lower()) for word in words if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
        "    corpus_lemetized.append(words)\n",
        "corpus_lemetized\n",
        "\n",
        "\n",
        "vocabulary = []\n",
        "for doc in corpus_lemetized:\n",
        "    for word in doc:\n",
        "        if word not in vocabulary:\n",
        "            vocabulary.append(word)\n",
        "vocabulary.sort()\n",
        "\n",
        "vocabulary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHlhd9RBn7Kq"
      },
      "source": [
        "* Construire une matrice carr√©e des cooccurrences (Terme √† Terme) pour  une fen√™tre de taille ùëõ en consid√©rant les n mots  avant et apr√®s le mot  central de la fen√™tre.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfoW0XGmn7Kq"
      },
      "outputs": [],
      "source": [
        "V=len(vocabulary)\n",
        "import numpy as np\n",
        "\n",
        "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
        "\n",
        "\n",
        "M=np.zeros((V,V))\n",
        "n=4\n",
        "for doc in corpus_lemetized:\n",
        "    T=len(doc)-2*n+1 if n<len(doc) else 1\n",
        "    for t in range(T):\n",
        "        borne=len(doc) if t+n>len(doc) else t+n\n",
        "        for w1 in doc[t:borne]:\n",
        "            for w2 in doc[t:borne]:\n",
        "                if w1!=w2:\n",
        "                    M[vocabulary.index(w1)][vocabulary.index(w2)]+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kd5uhGAn7Kr"
      },
      "source": [
        "* Appliquer La SVD pour obtenir une representation vectorielle des differents mots du vocabulaire dans un espace de dimension 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2a1-5qhn7Kr"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "svd = TruncatedSVD(n_components=2)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "vectors = lsa.fit_transform(M)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjjO8y8fn7Kr"
      },
      "source": [
        "* En se servant de la bibliotheque matplotlib Tracer les vecteurs obtenus dans un plan 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6GGArRin7Kr",
        "outputId": "68e3f6c3-2d2a-473e-d896-ddbccae21596"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "x=[M[i][0] for i in range(V)]\n",
        "y=[M[i][1] for i in range(V)]\n",
        "fig, ax = pyplot.subplots()\n",
        "ax.scatter(x, y)\n",
        "for i, txt in enumerate(vocabulary):\n",
        "    ax.annotate(txt, (x[i], y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlnr1f1jn7Kr"
      },
      "source": [
        "### Exercice\n",
        "\n",
        "\n",
        "* R√©cuperer le dataset du plagiarisme?\n",
        "* R√©aliser les diff√©rentes t√¢ches de pr√©traitement?\n",
        "* R√©cuperer la repr√©setation vectorielle des differents document du corpus\n",
        "* Appliquer La SVD pour reduire la dimesion de la representation vectorielle des termes pour les representer dans un espace de dimension 2\n",
        "* Tracer les vecteurs obtenus en se servant de la bibliotheque matplolib\n",
        "\n",
        "* En se servant de la repr√©sentation vectorielle obtenue, calculer les similarit√©s entre les r√©ponses des √©tudiants et les d√©finitions trouv√©es sur Wikip√©dia. R√©aliser une repr√©sentation vectorielle des documents en se basant sur les repr√©sentations vectorielles de leurs mots (une moyenne par exemple) et utiliser par la suite la distance euclidienne ou la distance corsinus.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOnmF9nin7Ks"
      },
      "source": [
        "## 3.3.\tApprochee iteratives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq-ByqlJn7Ks"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byQM2uKgn7Ks"
      },
      "source": [
        "Word2Vec est un algorithme √† base des r√©seaux de neurones et qui permet d'avoir une repr√©sentation vectorielle des mots contenus dans un corpus tr√®s large de documents texte de telle sorte que les mots qui se r√©p√®tent toujours ensemble dans les m√™mes contextes auront des repr√©sentations vectorielles similaires.\n",
        "\n",
        "L'algorithme word2Vect doit tourner sur un corpus tr√®s large de documents texte afin d'obtenir un mod√®le donnant une bonne repr√©sentation vectorielle d'un nombre important de mots. Cela n√©cessitera bien √©videment un temps consid√©rable pendant le processus d'apprentissage et n√©cessitera √©galement des ressources importantes en mati√®re de CPU et de RAM.\n",
        "\n",
        "La librairie Gensim fourni une impl√©mentation de l'algorithme Word2Vec avec des mod√®les pr√©√©tablis qui peuvent √™tre exploit√©s dans la comparaison de documents texte :\n",
        "\n",
        "    *fasttext-wiki-news-subwords-300\n",
        "    *conceptnet-numberbatch-17-06-300\n",
        "    *word2vec-ruscorpora-300\n",
        "    *word2vec-google-news-300\n",
        "    *glove-wiki-gigaword-50\n",
        "    *glove-wiki-gigaword-100\n",
        "    *glove-wiki-gigaword-200\n",
        "    *glove-wiki-gigaword-300\n",
        "    *glove-twitter-25\n",
        "    *glove-twitter-50\n",
        "    *glove-twitter-100\n",
        "    *glove-twitter-200\n",
        "\n",
        "Ci-dessous un code permettant de r√©cup√©rer le mod√®le pr√©√©tabli contenant 1193514 mots repr√©sent√©s dans un espace vectoriel de dimension 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcC3Ijnkn7Ks"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNnwAoPfn7Ks"
      },
      "source": [
        "Le mod√®le pr√©√©tablis peut √™tre utilis√© pour r√©cup√©rer les repr√©sentations vectorielles des mots comme ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO-ja-rLn7Kt"
      },
      "outputs": [],
      "source": [
        "vec_data = glove_vectors['data']\n",
        "print(vec_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEmU7ciXn7Kt"
      },
      "source": [
        "Genism offre plusieurs fonctions permettant de r√©cup√©rer et d'exploiter les similarit√©s entre les mots en se basant sur leurs repr√©sentations vectorielles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgvd-H9Gn7Kt"
      },
      "source": [
        "* Le code ci-deesous permet de recuperer les 10 terms les plus similaires √† un terme donn√©."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHaelbFvn7Kt"
      },
      "outputs": [],
      "source": [
        "glove_vectors.most_similar('data',topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slz-pnrSn7Ku"
      },
      "source": [
        "* Le code ci-dessous permet de r√©cup√©rer l'ordre de similarit√© entre deux termes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3aLnZ6un7Ku"
      },
      "outputs": [],
      "source": [
        "glove_vectors.similarity('data', 'information')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ4iaEi0n7Ku"
      },
      "source": [
        "* Le code ci-dessous permet de r√©cup√©rer le terme le moins convenable dans un ensemble de termes en se basant sur leurs similarit√©s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j23TAVUfn7Ku"
      },
      "outputs": [],
      "source": [
        "print(glove_vectors.doesnt_match(['data', 'information', 'processing', 'computer', 'car','machine','dashboard']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2loNKH3Cn7Kv"
      },
      "source": [
        "Bien √©videmment, on peut apprendre notre propre mod√®le en suivant les √©tapes ci-dessous\n",
        "\n",
        "   * R√©cup√©rer le corpus (Voir le code dans la section 1)\n",
        "   * R√©aliser les pr√©traitements n√©cessaires pour obtenir la liste des documents segment√©s: une liste de listes tq chaque sous listes comporte les mots d'un documents du corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv0ewdaFn7Kv"
      },
      "outputs": [],
      "source": [
        "corpus_lemetized=[]\n",
        "for doc in corpus:\n",
        "    words = word_tokenize(doc)\n",
        "    words = [lemmmatizer.lemmatize(word.lower()) for word in words if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
        "    corpus_lemetized.append(words)\n",
        "corpus_lemetized\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9mKFf9Ln7Kv"
      },
      "source": [
        "Pour g√©n√©rer le mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZoB9qFnn7Kv"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models.word2vec import PathLineSentences\n",
        "model = Word2Vec(sentences=corpus_lemetized, vector_size=10, window=3, min_count=1, workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elLVPLtYn7Kw"
      },
      "source": [
        "Pour r√©cup√©rer la representation vectorielle d'un mot:   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS0f0jren7Kw"
      },
      "outputs": [],
      "source": [
        "model.wv['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuwySRMPn7Kw"
      },
      "source": [
        "Pour r√©cup√©rer tous les vecteurs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdhavQHZn7Kw"
      },
      "outputs": [],
      "source": [
        "vectors=model.wv.vectors\n",
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr_V6QWbn7Kx"
      },
      "source": [
        "Recuperer tous le smots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXwPc9L9n7Kx"
      },
      "outputs": [],
      "source": [
        "words = model.wv.index_to_key\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnjIvzsOn7Kx"
      },
      "source": [
        "### Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEcqy7Pn7Ky"
      },
      "source": [
        "Doc2Vec est une impl√©mentation de l'algorithme Paragraph Vector qui est bas√© sur Word2Vec et qui est plus adapt√© √† la comparaison de documents texte en se basant sur leur repr√©sentations vectorielles.\n",
        "\n",
        "Cet algorithme d√©passe dans sa performance l‚Äôutilisation des moyenne des repr√©sentations vectorielles des mots pr√©sents dans un documents texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4ar-zvsn7Ky"
      },
      "source": [
        "* Pour g√©nerer le mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUy4PEkyn7Ky"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_lemetized)]\n",
        "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dPkDotyn7Kz"
      },
      "source": [
        "* Pour r√©cup√©rer la representation vectorielle de tous les documents   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWOPdLgyn7Kz"
      },
      "outputs": [],
      "source": [
        "vectors=model.dv.vectors\n",
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmjdjvr1n7Kz"
      },
      "source": [
        "Pour r√©cup√©rer la les labels de tous les documents   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0A43I9vn7Kz"
      },
      "outputs": [],
      "source": [
        "labels=model.dv.index_to_key\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7kBN8gAn7Kz"
      },
      "source": [
        "Pour r√©cup√©rer la repr√©sentation vectorielle d'un document particulier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qplxGLPPn7K0"
      },
      "outputs": [],
      "source": [
        "model.dv[80]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHJFQpDXn7K0"
      },
      "source": [
        "### Exercice\n",
        "\n",
        "\n",
        "* En se servant de la repr√©sentation vectorielle doc2vec, calculer les similarit√©s entre les r√©ponses des √©tudiants et les d√©finitions trouv√©es sur Wikip√©dia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niLAZaZ7n7K0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}