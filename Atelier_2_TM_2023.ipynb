{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW4ZxVd1n7Kd"
      },
      "source": [
        "# Atelier 2 : Vectorisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT1wV-Don7Kf"
      },
      "source": [
        "## 1.\tObjectif\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P20HNzoCn7Kf"
      },
      "source": [
        "L’objectif de cet atelier est de decouvrire les techniques courantes de vectorisation de documents text ansi que les differents algorithmes permettant de caculer la similarité ou la distance entre les document texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aCCUzLyn7Kg"
      },
      "source": [
        "## 2. Méthodes Basiques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS-xpif2n7Kg"
      },
      "source": [
        "Afin de simplifier l’analyse des données texte, il est recommandé d’utiliser des représentations plus consistantes qu’une simple segmentation. Il faut bien évidement réaliser des prétraitements telles que la racinisation, la lemmatisation, supprimer les redondances, supprimer les mots qui représentent le même sens. Mais c’est encore insuffisant pour obtenir un modèle représentatif qui reflète l’importance et le sens exacte de chaque mot dans une expression ou dans un document texte.\n",
        "Afin de repondre à ce besoin, plusieurs représentations vectorielles des termes contenus dans un texte sont possibles : one-hot-vector, Bag-of-words, TF-IDF, SVD, Word2vec... Nous utilisant scikitlearn pour réaliser ces différentes représentations vectorielles. Ça n’empêche pas que ces représentations vectorielles peuvent être obtenues en faisant du codage from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ITkj6bbn7Kh"
      },
      "source": [
        "## 2.1. One-hot-vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHi6XziRn7Kh"
      },
      "source": [
        "Le modèle commence par la création d’un vocabulaire à partir du corpus formé par tous les documents ou les expressions texte et determine par la suite pour chaque document/expression la présence de chaque terme du vocabulaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFpfILnQn7Kh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "freq   = CountVectorizer()\n",
        "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
        "corpus=[ sent.lower() for sent in corpus]\n",
        "corpus = freq.fit_transform(corpus)\n",
        "\n",
        "onehot = Binarizer()\n",
        "corpus = onehot.fit_transform(corpus.toarray())\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUpeuGeSn7Ki"
      },
      "source": [
        "## 2.2.\tBeg-of-words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGz_XGYvn7Kj"
      },
      "source": [
        "Pour la représentation vectorielle Bag-of-Words, le modèle commence par la création d’un vocabulaire à partir du corpus formé par tous les documents ou les expressions texte et calcul par la suite pour chaque document/expression le nombre d’occurrences de chaque terme du vocabulaire.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNIDwQCxn7Kj",
        "outputId": "60b44742-e88b-4bf7-c04e-50b74301f764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 1 0 1 0 1 1 0 1]\n",
            " [1 0 0 0 1 0 1 1 0]\n",
            " [0 1 1 1 0 0 1 0 1]]\n",
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = ['This is the first document.','This is the second  document.','And the third one.','Is this the first document?']\n",
        "corpus=[ sent.lower() for sent in corpus]\n",
        "X = vectorizer.fit_transform(corpus) #sparsy format\n",
        "print(X.toarray()) # explicit matrix format\n",
        "print(vectorizer.get_feature_names() ) #vocabulary as list of string\n",
        "print(vectorizer.vocabulary_.get('document')) #get column index of a specific term in the vocabulary\n",
        "vectorizer.transform(['Something completely new.']).toarray()#apply the model to a new document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0zkMHOin7Kj"
      },
      "source": [
        "## 2.3.\tTF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCqlEza-n7Kj"
      },
      "source": [
        "La représentation vectorielle Bag-of-Words considère la fréquence d’apparition des termes du vocabulaire dans chaque document du corpus séparément des autres. Cette représentation néglige l’importance du terme par rapport au corpus tout en entier.  TF-IDF (term-frequency times inverse document-frequency) est un autre modèle de représentation vectorielle des occurrences des termes d’un document en considérant également leurs occurrences dans tout le corpus. Cette approche va permettre de diminuer l’importance des termes les plus fréquents dans des documents texte tels que les stop words.\n",
        "\n",
        "tf-idf(t,d)=tf(t,d)×idf(t).\n",
        "\n",
        "idf(t)=log[(1+n)/(1+df(t))]+1\n",
        "n: la taille du corpus.\n",
        "df(t) :  le nombre de documents qui comportent le terme t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXn0f4ACn7Kk",
        "outputId": "8019d6dd-8cc1-412a-e3fc-e45afa4935db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
            "  0.35872874 0.         0.43877674]\n",
            " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
            "  0.22262429 0.         0.27230147]\n",
            " [0.55280532 0.         0.         0.         0.55280532 0.\n",
            "  0.28847675 0.55280532 0.        ]\n",
            " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
            "  0.35872874 0.         0.43877674]]\n",
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = ['This is the first document.','this is the second second Document.','And the third one.','Is this the first document?']\n",
        "vectorizer = TfidfVectorizer()\n",
        "X=vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        "print(vectorizer.get_feature_names() )\n",
        "\n",
        "#NB: le vecteur tf-idf obtenu sera normalisé pour obtenir des valeurs entre 0 et 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_htBOtQn7Kk"
      },
      "source": [
        "## 2.4. Exercice 1: Detection du plagiarisme Approche syntaxique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBr2xsRZn7Kk"
      },
      "source": [
        "On va reprendre l'exercie de detection du plagiarisme en se bsant sur une representation vectorielle des document du corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaSxi4MQn7Kk"
      },
      "source": [
        "Pour la similarité syntaxique entre des vecteurs, plusieurs distances sont possibles à savoir : distance, euclidienne, Cosine Jaccard, Levenshtein, Hamming…\n",
        "\n",
        "l'exemple ci-dessous permet de calculer la similarité syntaxique entre les documents en se basant sur une representation vectorielle en TFIDF avec la distance euclidienne et la distance cosine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ghsOP4mn7Kk",
        "outputId": "90940c60-4a34-4a2a-b28c-b32bbc462761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         1.05990529 1.33904078 0.        ]]\n",
            "[[0.         0.56169962 0.8965151  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances,euclidean_distances\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "tfidf_matrix.shape\n",
        "\n",
        "#compute similarity for first sentence with rest of the sentences\n",
        "print(euclidean_distances(tfidf_matrix[0:1],tfidf_matrix))\n",
        "print(cosine_distances(tfidf_matrix[0:1],tfidf_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmEHJhf2n7Kl"
      },
      "source": [
        "* Realiser la même chose en se basant sur les representations OHV et BOW?\n",
        "* Comparer les performances des trois methodes en terme de temps d'execution et precision?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F02K82EFn7Km"
      },
      "source": [
        "# 3.\tMethodes Avancées\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wopGemCzn7Kp"
      },
      "source": [
        "* Installer Gensim: pip install --upgrade gensim\n",
        "* Recuperer le dataset du plagiarisme?\n",
        "* Réaliser les différentes tâches de prétraitement?\n",
        "* Generer le vocabulaire sous forme d'une liste de termes distincts est triés dans l'ordre lexicograpique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp7kUitTn7Kp"
      },
      "source": [
        "## 3.2.\tApproche à base des Cooccurrences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJTgD2S8n7Kp"
      },
      "source": [
        "* Generer le vocabulaire sous forme d'une liste de termes distincts est triés dans l'ordre lexicograpique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9pGmPqgn7Kp",
        "outputId": "5217cd4e-3b5a-4269-a904-2efa5a50fe9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['and', 'document', 'first', 'is', 'one', 'second', 'third', 'this']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "lemmmatizer=WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
        "corpus_lemetized=[]\n",
        "\n",
        "for doc in corpus:\n",
        "    words = word_tokenize(doc)\n",
        "    words = [lemmmatizer.lemmatize(word.lower()) for word in words if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
        "    corpus_lemetized.append(words)\n",
        "corpus_lemetized\n",
        "\n",
        "\n",
        "vocabulary = []\n",
        "for doc in corpus_lemetized:\n",
        "    for word in doc:\n",
        "        if word not in vocabulary:\n",
        "            vocabulary.append(word)\n",
        "vocabulary.sort()\n",
        "\n",
        "vocabulary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHlhd9RBn7Kq"
      },
      "source": [
        "* Construire une matrice carrée des cooccurrences (Terme à Terme) pour  une fenêtre de taille 𝑛 en considérant les n mots  avant et après le mot  central de la fenêtre.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfoW0XGmn7Kq"
      },
      "outputs": [],
      "source": [
        "V=len(vocabulary)\n",
        "import numpy as np\n",
        "\n",
        "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
        "\n",
        "\n",
        "M=np.zeros((V,V))\n",
        "n=4\n",
        "for doc in corpus_lemetized:\n",
        "    T=len(doc)-2*n+1 if n<len(doc) else 1\n",
        "    for t in range(T):\n",
        "        borne=len(doc) if t+n>len(doc) else t+n\n",
        "        for w1 in doc[t:borne]:\n",
        "            for w2 in doc[t:borne]:\n",
        "                if w1!=w2:\n",
        "                    M[vocabulary.index(w1)][vocabulary.index(w2)]+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kd5uhGAn7Kr"
      },
      "source": [
        "* Appliquer La SVD pour obtenir une representation vectorielle des differents mots du vocabulaire dans un espace de dimension 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2a1-5qhn7Kr"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "svd = TruncatedSVD(n_components=2)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "vectors = lsa.fit_transform(M)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjjO8y8fn7Kr"
      },
      "source": [
        "* En se servant de la bibliotheque matplotlib Tracer les vecteurs obtenus dans un plan 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6GGArRin7Kr",
        "outputId": "68e3f6c3-2d2a-473e-d896-ddbccae21596"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "x=[M[i][0] for i in range(V)]\n",
        "y=[M[i][1] for i in range(V)]\n",
        "fig, ax = pyplot.subplots()\n",
        "ax.scatter(x, y)\n",
        "for i, txt in enumerate(vocabulary):\n",
        "    ax.annotate(txt, (x[i], y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlnr1f1jn7Kr"
      },
      "source": [
        "### Exercice\n",
        "\n",
        "\n",
        "* Récuperer le dataset du plagiarisme?\n",
        "* Réaliser les différentes tâches de prétraitement?\n",
        "* Récuperer la représetation vectorielle des differents document du corpus\n",
        "* Appliquer La SVD pour reduire la dimesion de la representation vectorielle des termes pour les representer dans un espace de dimension 2\n",
        "* Tracer les vecteurs obtenus en se servant de la bibliotheque matplolib\n",
        "\n",
        "* En se servant de la représentation vectorielle obtenue, calculer les similarités entre les réponses des étudiants et les définitions trouvées sur Wikipédia. Réaliser une représentation vectorielle des documents en se basant sur les représentations vectorielles de leurs mots (une moyenne par exemple) et utiliser par la suite la distance euclidienne ou la distance corsinus.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOnmF9nin7Ks"
      },
      "source": [
        "## 3.3.\tApprochee iteratives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq-ByqlJn7Ks"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byQM2uKgn7Ks"
      },
      "source": [
        "Word2Vec est un algorithme à base des réseaux de neurones et qui permet d'avoir une représentation vectorielle des mots contenus dans un corpus très large de documents texte de telle sorte que les mots qui se répètent toujours ensemble dans les mêmes contextes auront des représentations vectorielles similaires.\n",
        "\n",
        "L'algorithme word2Vect doit tourner sur un corpus très large de documents texte afin d'obtenir un modèle donnant une bonne représentation vectorielle d'un nombre important de mots. Cela nécessitera bien évidement un temps considérable pendant le processus d'apprentissage et nécessitera également des ressources importantes en matière de CPU et de RAM.\n",
        "\n",
        "La librairie Gensim fourni une implémentation de l'algorithme Word2Vec avec des modèles préétablis qui peuvent être exploités dans la comparaison de documents texte :\n",
        "\n",
        "    *fasttext-wiki-news-subwords-300\n",
        "    *conceptnet-numberbatch-17-06-300\n",
        "    *word2vec-ruscorpora-300\n",
        "    *word2vec-google-news-300\n",
        "    *glove-wiki-gigaword-50\n",
        "    *glove-wiki-gigaword-100\n",
        "    *glove-wiki-gigaword-200\n",
        "    *glove-wiki-gigaword-300\n",
        "    *glove-twitter-25\n",
        "    *glove-twitter-50\n",
        "    *glove-twitter-100\n",
        "    *glove-twitter-200\n",
        "\n",
        "Ci-dessous un code permettant de récupérer le modèle préétabli contenant 1193514 mots représentés dans un espace vectoriel de dimension 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcC3Ijnkn7Ks"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNnwAoPfn7Ks"
      },
      "source": [
        "Le modèle préétablis peut être utilisé pour récupérer les représentations vectorielles des mots comme ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO-ja-rLn7Kt"
      },
      "outputs": [],
      "source": [
        "vec_data = glove_vectors['data']\n",
        "print(vec_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEmU7ciXn7Kt"
      },
      "source": [
        "Genism offre plusieurs fonctions permettant de récupérer et d'exploiter les similarités entre les mots en se basant sur leurs représentations vectorielles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgvd-H9Gn7Kt"
      },
      "source": [
        "* Le code ci-deesous permet de recuperer les 10 terms les plus similaires à un terme donné."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHaelbFvn7Kt"
      },
      "outputs": [],
      "source": [
        "glove_vectors.most_similar('data',topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slz-pnrSn7Ku"
      },
      "source": [
        "* Le code ci-dessous permet de récupérer l'ordre de similarité entre deux termes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3aLnZ6un7Ku"
      },
      "outputs": [],
      "source": [
        "glove_vectors.similarity('data', 'information')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ4iaEi0n7Ku"
      },
      "source": [
        "* Le code ci-dessous permet de récupérer le terme le moins convenable dans un ensemble de termes en se basant sur leurs similarités."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j23TAVUfn7Ku"
      },
      "outputs": [],
      "source": [
        "print(glove_vectors.doesnt_match(['data', 'information', 'processing', 'computer', 'car','machine','dashboard']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2loNKH3Cn7Kv"
      },
      "source": [
        "Bien évidemment, on peut apprendre notre propre modèle en suivant les étapes ci-dessous\n",
        "\n",
        "   * Récupérer le corpus (Voir le code dans la section 1)\n",
        "   * Réaliser les prétraitements nécessaires pour obtenir la liste des documents segmentés: une liste de listes tq chaque sous listes comporte les mots d'un documents du corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv0ewdaFn7Kv"
      },
      "outputs": [],
      "source": [
        "corpus_lemetized=[]\n",
        "for doc in corpus:\n",
        "    words = word_tokenize(doc)\n",
        "    words = [lemmmatizer.lemmatize(word.lower()) for word in words if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
        "    corpus_lemetized.append(words)\n",
        "corpus_lemetized\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9mKFf9Ln7Kv"
      },
      "source": [
        "Pour générer le modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZoB9qFnn7Kv"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models.word2vec import PathLineSentences\n",
        "model = Word2Vec(sentences=corpus_lemetized, vector_size=10, window=3, min_count=1, workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elLVPLtYn7Kw"
      },
      "source": [
        "Pour récupérer la representation vectorielle d'un mot:   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS0f0jren7Kw"
      },
      "outputs": [],
      "source": [
        "model.wv['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuwySRMPn7Kw"
      },
      "source": [
        "Pour récupérer tous les vecteurs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdhavQHZn7Kw"
      },
      "outputs": [],
      "source": [
        "vectors=model.wv.vectors\n",
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr_V6QWbn7Kx"
      },
      "source": [
        "Recuperer tous le smots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXwPc9L9n7Kx"
      },
      "outputs": [],
      "source": [
        "words = model.wv.index_to_key\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnjIvzsOn7Kx"
      },
      "source": [
        "### Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEcqy7Pn7Ky"
      },
      "source": [
        "Doc2Vec est une implémentation de l'algorithme Paragraph Vector qui est basé sur Word2Vec et qui est plus adapté à la comparaison de documents texte en se basant sur leur représentations vectorielles.\n",
        "\n",
        "Cet algorithme dépasse dans sa performance l’utilisation des moyenne des représentations vectorielles des mots présents dans un documents texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4ar-zvsn7Ky"
      },
      "source": [
        "* Pour génerer le modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUy4PEkyn7Ky"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_lemetized)]\n",
        "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dPkDotyn7Kz"
      },
      "source": [
        "* Pour récupérer la representation vectorielle de tous les documents   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWOPdLgyn7Kz"
      },
      "outputs": [],
      "source": [
        "vectors=model.dv.vectors\n",
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmjdjvr1n7Kz"
      },
      "source": [
        "Pour récupérer la les labels de tous les documents   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0A43I9vn7Kz"
      },
      "outputs": [],
      "source": [
        "labels=model.dv.index_to_key\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7kBN8gAn7Kz"
      },
      "source": [
        "Pour récupérer la représentation vectorielle d'un document particulier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qplxGLPPn7K0"
      },
      "outputs": [],
      "source": [
        "model.dv[80]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHJFQpDXn7K0"
      },
      "source": [
        "### Exercice\n",
        "\n",
        "\n",
        "* En se servant de la représentation vectorielle doc2vec, calculer les similarités entre les réponses des étudiants et les définitions trouvées sur Wikipédia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niLAZaZ7n7K0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}